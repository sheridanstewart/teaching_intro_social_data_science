{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ce3ad7c",
   "metadata": {},
   "source": [
    "#### Sociology 128D: Mining Culture Through Text Data: Introduction to Social Data Science â€“ Summer '22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fbfe19",
   "metadata": {},
   "source": [
    "# Notebook 8: Modeling Ratings and Sentiment using Python\n",
    "\n",
    "In this notebook, we are going to explore a range of tools for statistical analysis in Python. To do this, we are going to use the text of Yelp reviews as well as various metadata. We will conclude with regression tables that can be displayed in the notebook, copied to a word processor, or rendered in HTML or LaTeX. In the optional exercises, you will adapt the code provided in the notebook to answer your own research question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dde7c4",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "You will likely need to install `contractions`,  `num2words`, `pingouin`, `stargazer`, `statsmodels`, `unidecode`, and `vaderSentiment`. You can install most of these using `conda` (if using Anaconda), but you will need to install `contractions` and `stargazer` using `pip` (see below).\n",
    "\n",
    "`conda install -c conda-forge pingouin num2words statsmodels unidecode vadersentiment`\n",
    "\n",
    "\n",
    "`pip install contractions` <br>\n",
    "`pip install stargazer`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01fc2d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pingouin\n",
    "import re\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from num2words import num2words\n",
    "from pingouin import cronbach_alpha\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import classification_report, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as spacy_stopwords\n",
    "from stargazer.stargazer import Stargazer\n",
    "from statsmodels.stats.weightstats import ttest_ind\n",
    "from unidecode import unidecode\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82fb491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2423)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b995b7",
   "metadata": {},
   "source": [
    "## I. Data\n",
    "\n",
    "For this notebook, we are going to use the Yelp Open Dataset, which you can find [here](https://www.yelp.com/dataset). You'll have to click 'Download Dataset', agree to the terms, and click 'Download JSON'. It's a large download: ~5GB compressed and ~11GB once you've uncompressed it. The dataset has 8,635,403 reviews of businesses including text, a rating out of five stars, and various other information. The main file, <tt>yelp_academic_dataset_review.json</tt>, is quite large, and we are going to take a random sample of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a841c668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dataset_User_Agreement.pdf',\n",
       " 'yelp_academic_dataset_business.json',\n",
       " 'yelp_academic_dataset_checkin.json',\n",
       " 'yelp_academic_dataset_review.json',\n",
       " 'yelp_academic_dataset_tip.json',\n",
       " 'yelp_academic_dataset_user.json']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"data/yelp_dataset/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165218e",
   "metadata": {},
   "source": [
    "First, let's confirm the number of reviews. We'll iterate through the file line by line, counting the lines by incrementing <tt>num_reviews</tt>. Then we'll use `np.random.choice` to identify the *indices* of random elements from an array of the same length. Then we'll loop back through the lines of the main file and keep the lines whose index is a match for our sample. This approach allows us to avoid loading the full dataset into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd769e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_reviews = 0\n",
    "\n",
    "with open(\"data/yelp_dataset/yelp_academic_dataset_review.json\", \"r\", encoding=\"utf-8\") as reader:\n",
    "    for line in reader:\n",
    "        num_reviews += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdfea4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6990280"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b848e0ef",
   "metadata": {},
   "source": [
    "Using `np.arange` with <tt>num_reviews</tt> gives us an array of the same length as <tt>num_reviews</tt> from 0 to <tt>num_reviews</tt>-1. This is more or less doing the same thing as the more familiar `range` function, but it returns a NumPy array. You can use either, but `np.arange` is a good tool to have if you start working with NumPy more.\n",
    "\n",
    "`np.random.choice` will take a sample of the values from the array of size <tt>size</tt> without replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f243f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_indices = np.random.choice(np.arange(num_reviews), size=10000, replace=False)\n",
    "len(sample_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79d6eb",
   "metadata": {},
   "source": [
    "Now, we will loop through the lines of the main file using `enumerate` to count as we go. For the first line, <tt>i</tt> will be 0. For the last line, <tt>i</tt> will be 8635402. For each line, if the corresponding value of <tt>i</tt> is in <tt>sample_indices</tt>, we will append the line to the list we've called <tt>sample</tt>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48fbbed8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sample = []\n",
    "\n",
    "with open(\"data/yelp_dataset/yelp_academic_dataset_review.json\", \"r\", encoding=\"utf-8\") as reader:\n",
    "    for i, line in enumerate(reader):\n",
    "        if i in sample_indices:\n",
    "            sample.append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68021842",
   "metadata": {},
   "source": [
    "JSON is a bit like a dictionary. If we use `json.loads`, we can turn each line (a string) into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2ad7751",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [json.loads(s) for s in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c8fe626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review_id': 'sgvNB37eQ9wJYOBv1lmwDA',\n",
       " 'user_id': 'DU_duJ_ZqG17lsZWGaIOuw',\n",
       " 'business_id': 'ziFtaIQdzQfFL79iLTq_zQ',\n",
       " 'stars': 3.0,\n",
       " 'useful': 0,\n",
       " 'funny': 0,\n",
       " 'cool': 0,\n",
       " 'text': \"not a bad little burger joint. i got the bacon cheeseburger. bacon was cooked perfect and had the perfect thickness. bun had a really great toasted crust. fries were good, as well as the chocolate shake I had. 3 complaints, i wish the burger was more juicy, I wish I wasn't paying so much for the tiny burger portion I was served, and I wish I didn't leave the place smelling like burger grease.\",\n",
       " 'date': '2016-11-10 00:35:28'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b643153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0][\"stars\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3caa4ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"not a bad little burger joint. i got the bacon cheeseburger. bacon was cooked perfect and had the perfect thickness. bun had a really great toasted crust. fries were good, as well as the chocolate shake I had. 3 complaints, i wish the burger was more juicy, I wish I wasn't paying so much for the tiny burger portion I was served, and I wish I didn't leave the place smelling like burger grease.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed96c4f7",
   "metadata": {},
   "source": [
    "<tt>sample</tt> is a list of 10,000 dictionaries, each corresponding to one review. However, each fo these dictionaries has the same keys. We can use `pd.Dataframe` directly on this list to create a dataframe, and it will use the keys of the dictionaries as the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5df5c242",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47d6b9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sgvNB37eQ9wJYOBv1lmwDA</td>\n",
       "      <td>DU_duJ_ZqG17lsZWGaIOuw</td>\n",
       "      <td>ziFtaIQdzQfFL79iLTq_zQ</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>not a bad little burger joint. i got the bacon...</td>\n",
       "      <td>2016-11-10 00:35:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_zfr4ImlsKPKO7NClwlDvA</td>\n",
       "      <td>1OkMhU3AfrTGiXmq-q0AiA</td>\n",
       "      <td>skJ1w8B5YusIibyxH4ohxQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Our office went today for Friday's happy hour ...</td>\n",
       "      <td>2014-10-11 04:56:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bJk7Y9I0olICC2hfoZoj6w</td>\n",
       "      <td>Vlq8ZFfqA3FWTmjz_C3jcg</td>\n",
       "      <td>zjNcjqgQlPrbnn-pV5YCSg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>High pressure tea sales!  Good teas but outrag...</td>\n",
       "      <td>2012-05-28 22:22:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dWotGplxOFvATs92qiAvuA</td>\n",
       "      <td>kRZYT3eiOg3C-wrPNgyMQw</td>\n",
       "      <td>O6RCCmz8-x4NhWJpI8KQug</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Food and coffee are great. Just have 30\\nMinut...</td>\n",
       "      <td>2016-07-03 17:13:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tA_eAuE5Tk6oW6Ob6srujw</td>\n",
       "      <td>3zeNqvWimKiA9gOSbx2e6Q</td>\n",
       "      <td>GBTPC53ZrG1ZBY3DT8Mbcw</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazing! Late night arrival in NOLA so got ama...</td>\n",
       "      <td>2017-12-16 01:20:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  sgvNB37eQ9wJYOBv1lmwDA  DU_duJ_ZqG17lsZWGaIOuw  ziFtaIQdzQfFL79iLTq_zQ   \n",
       "1  _zfr4ImlsKPKO7NClwlDvA  1OkMhU3AfrTGiXmq-q0AiA  skJ1w8B5YusIibyxH4ohxQ   \n",
       "2  bJk7Y9I0olICC2hfoZoj6w  Vlq8ZFfqA3FWTmjz_C3jcg  zjNcjqgQlPrbnn-pV5YCSg   \n",
       "3  dWotGplxOFvATs92qiAvuA  kRZYT3eiOg3C-wrPNgyMQw  O6RCCmz8-x4NhWJpI8KQug   \n",
       "4  tA_eAuE5Tk6oW6Ob6srujw  3zeNqvWimKiA9gOSbx2e6Q  GBTPC53ZrG1ZBY3DT8Mbcw   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0    3.0       0      0     0   \n",
       "1    5.0       1      0     1   \n",
       "2    1.0       0      0     0   \n",
       "3    3.0       2      1     0   \n",
       "4    5.0       0      0     0   \n",
       "\n",
       "                                                text                 date  \n",
       "0  not a bad little burger joint. i got the bacon...  2016-11-10 00:35:28  \n",
       "1  Our office went today for Friday's happy hour ...  2014-10-11 04:56:46  \n",
       "2  High pressure tea sales!  Good teas but outrag...  2012-05-28 22:22:10  \n",
       "3  Food and coffee are great. Just have 30\\nMinut...  2016-07-03 17:13:37  \n",
       "4  Amazing! Late night arrival in NOLA so got ama...  2017-12-16 01:20:58  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "925c412e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2005-07-25 21:44:13'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.date.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af34e7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-01-19 00:59:27'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.date.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2acb6f",
   "metadata": {},
   "source": [
    "Some quick preprocessing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f509ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_ordinal_nums(word: str) -> str:\n",
    "    ord_num_reg = r\"\\d+[(st)(nd)(rd)(th)]\"\n",
    "    try:\n",
    "        if any(re.findall(ord_num_reg, word)):\n",
    "            word = re.sub(\"[(st)(nd)(rd)(th)]\", \"\", word)\n",
    "            word = num2words(word, lang=\"en\", to=\"ordinal\")\n",
    "            \n",
    "        return word\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        return word\n",
    "\n",
    "\n",
    "def preprocess_post(post: str) -> str:\n",
    "    \"\"\"\n",
    "    Tokenize, lemmatize, remove stop words, \n",
    "    remove non-alphabetic characters.\n",
    "    \"\"\"\n",
    "    post = unidecode(str(post))\n",
    "    post = contractions.fix(post)\n",
    "    post = [word.lemma_ for word in nlp(post) if (word.text not in spacy_stopwords) & (len(word.text) > 1)]\n",
    "    post = \" \".join([fix_ordinal_nums(word) for word in post])\n",
    "    post = re.sub(\"[^a-z]\", \" \", post.lower())\n",
    "    \n",
    "    return re.sub(\"\\s+\", \" \", post).strip()\n",
    "    \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fad2887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%time df[\"preprocessed\"] = df.text.apply(preprocess_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16721b90",
   "metadata": {},
   "source": [
    "We aren't going to use the <tt>user_id</tt> column, and <tt>review_id</tt> is also an eyesore when we try to display the dataframe. Let's drop those columns and save our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcfd39c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(labels=[\"review_id\", \"user_id\"], axis=1, inplace=True)\n",
    "df.to_json(\"yelp_sample.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36a2396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"yelp_sample.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b6d8b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ziFtaIQdzQfFL79iLTq_zQ</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>not a bad little burger joint. i got the bacon...</td>\n",
       "      <td>2016-11-10 00:35:28</td>\n",
       "      <td>bad little burger joint get bacon cheeseburger...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>skJ1w8B5YusIibyxH4ohxQ</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Our office went today for Friday's happy hour ...</td>\n",
       "      <td>2014-10-11 04:56:46</td>\n",
       "      <td>our office go today friday happy hour special ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zjNcjqgQlPrbnn-pV5YCSg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>High pressure tea sales!  Good teas but outrag...</td>\n",
       "      <td>2012-05-28 22:22:10</td>\n",
       "      <td>high pressure tea sale good tea outrageously e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O6RCCmz8-x4NhWJpI8KQug</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Food and coffee are great. Just have 30\\nMinut...</td>\n",
       "      <td>2016-07-03 17:13:37</td>\n",
       "      <td>food coffee great just minutes stand line wait...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GBTPC53ZrG1ZBY3DT8Mbcw</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazing! Late night arrival in NOLA so got ama...</td>\n",
       "      <td>2017-12-16 01:20:58</td>\n",
       "      <td>amazing late night arrival nola get amazing se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  stars  useful  funny  cool  \\\n",
       "0  ziFtaIQdzQfFL79iLTq_zQ      3       0      0     0   \n",
       "1  skJ1w8B5YusIibyxH4ohxQ      5       1      0     1   \n",
       "2  zjNcjqgQlPrbnn-pV5YCSg      1       0      0     0   \n",
       "3  O6RCCmz8-x4NhWJpI8KQug      3       2      1     0   \n",
       "4  GBTPC53ZrG1ZBY3DT8Mbcw      5       0      0     0   \n",
       "\n",
       "                                                text                date  \\\n",
       "0  not a bad little burger joint. i got the bacon... 2016-11-10 00:35:28   \n",
       "1  Our office went today for Friday's happy hour ... 2014-10-11 04:56:46   \n",
       "2  High pressure tea sales!  Good teas but outrag... 2012-05-28 22:22:10   \n",
       "3  Food and coffee are great. Just have 30\\nMinut... 2016-07-03 17:13:37   \n",
       "4  Amazing! Late night arrival in NOLA so got ama... 2017-12-16 01:20:58   \n",
       "\n",
       "                                        preprocessed  \n",
       "0  bad little burger joint get bacon cheeseburger...  \n",
       "1  our office go today friday happy hour special ...  \n",
       "2  high pressure tea sale good tea outrageously e...  \n",
       "3  food coffee great just minutes stand line wait...  \n",
       "4  amazing late night arrival nola get amazing se...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a460d1",
   "metadata": {},
   "source": [
    "#### Business Metadata\n",
    "\n",
    "Now we're going to fetch some info about the businesses themselves. First, we'll create a variable storing all of the unique business IDs in our sample. Next, we'll loop through the <tt>yelp_academic_dataset_business.json</tt> file and get info about the businesses whose IDs are in our sample.\n",
    "\n",
    "The <tt>categories</tt> and <tt>attributes</tt> fields could be useful. They provide a lot of information about different properties of the businesses, from accessibility to ambience to type of business. <tt>categories</tt> is provided as a single string, so we're going to split that into individual categories. <tt>attributes</tt> has fewer total values, but the results are dictionaries. Some are True/False (e.g., 'Outdoor Seating') and some are more varied values (e.g., 'hours'). One of the attribute dictionaries, 'Ambience', has key/value pairs indicating whether a business is touristy, 'hipster', romantic, divey, intimate, trendy, upscale, classy, and/or casual (each True/False). We'll treat each of those as a separate attribute because we're going to use them in our analyses below. If you are interested in some of the other attributes, you may need to modify the code below to handle them how 'Ambience' is handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df1afa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses_in_sample = df.business_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48f99e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8396"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(businesses_in_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c960a",
   "metadata": {},
   "source": [
    "Let's take a look at the <tt>categories</tt> and <tt>attributes</tt> fields. The code below prints the results for the first review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8d6ddf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doctors, Traditional Chinese Medicine, Naturopathic/Holistic, Acupuncture, Health & Medical, Nutritionists\n",
      "\n",
      "{'ByAppointmentOnly': 'True'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/yelp_dataset/yelp_academic_dataset_business.json\", \"r\", encoding=\"utf-8\") as reader:\n",
    "    for line in reader:\n",
    "        line = json.loads(line.strip())\n",
    "        print(line[\"categories\"])\n",
    "        print()\n",
    "        print(line[\"attributes\"])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb66841d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 51.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "biz_categories_dict = defaultdict(lambda: {})\n",
    "biz_attributes_dict = defaultdict(lambda: {})\n",
    "\n",
    "with open(\"data/yelp_dataset/yelp_academic_dataset_business.json\", \"r\", encoding=\"utf-8\") as reader:\n",
    "    for line in reader:\n",
    "        line = json.loads(line.strip())\n",
    "        biz_id = line[\"business_id\"]\n",
    "        if biz_id in businesses_in_sample:\n",
    "            categories = line[\"categories\"]\n",
    "            attributes = line[\"attributes\"]\n",
    "            \n",
    "            # check if \"categories\" is empty/None\n",
    "            if categories:\n",
    "                categories = categories.lower().split(\",\")\n",
    "                categories = [cat.strip() for cat in categories]\n",
    "                biz_categories_dict[biz_id] = set(categories)\n",
    "            \n",
    "            # check if \"attributes\" is empty/None\n",
    "            if attributes:\n",
    "                for key, value in attributes.items():\n",
    "                    if key == \"Ambience\":\n",
    "                        amb_categories = eval(value)\n",
    "                        if type(amb_categories) == dict:\n",
    "                            for cat, cat_val in amb_categories.items():\n",
    "                                biz_attributes_dict[cat][biz_id] = cat_val\n",
    "                    else:\n",
    "                        biz_attributes_dict[key][biz_id] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bf289a",
   "metadata": {},
   "source": [
    "Let's put all of the categories from <tt>categories</tt> together, count their frequencies (using `Counter`), and take a look at the most frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "acf62096",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_categories = []\n",
    "for cats in biz_categories_dict.values():\n",
    "    all_categories += cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b40f84ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "923"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(all_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "849437e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(all_categories)\n",
    "c = sorted(list(c.items()), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dfb74f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('restaurants', 5430),\n",
       " ('food', 2125),\n",
       " ('nightlife', 1719),\n",
       " ('bars', 1629),\n",
       " ('american (traditional)', 1064),\n",
       " ('american (new)', 992),\n",
       " ('breakfast & brunch', 862),\n",
       " ('sandwiches', 810),\n",
       " ('event planning & services', 752),\n",
       " ('seafood', 628),\n",
       " ('shopping', 622),\n",
       " ('pizza', 601),\n",
       " ('italian', 555),\n",
       " ('coffee & tea', 527),\n",
       " ('burgers', 511),\n",
       " ('mexican', 491),\n",
       " ('beauty & spas', 458),\n",
       " ('arts & entertainment', 416),\n",
       " ('salad', 408),\n",
       " ('hotels & travel', 380),\n",
       " ('fast food', 359),\n",
       " ('cocktail bars', 340),\n",
       " ('desserts', 334),\n",
       " ('automotive', 332),\n",
       " ('home services', 311),\n",
       " ('cafes', 310),\n",
       " ('specialty food', 295),\n",
       " ('health & medical', 277),\n",
       " ('caterers', 274),\n",
       " ('chinese', 274),\n",
       " ('wine & spirits', 270),\n",
       " ('beer', 270),\n",
       " ('venues & event spaces', 263),\n",
       " ('active life', 262),\n",
       " ('sushi bars', 253),\n",
       " ('pubs', 249),\n",
       " ('bakeries', 248),\n",
       " ('steakhouses', 247),\n",
       " ('local services', 247),\n",
       " ('sports bars', 241),\n",
       " ('japanese', 231),\n",
       " ('chicken wings', 228),\n",
       " ('hotels', 221),\n",
       " ('asian fusion', 221),\n",
       " ('wine bars', 212),\n",
       " ('barbeque', 206),\n",
       " ('vegetarian', 197),\n",
       " ('cajun/creole', 193),\n",
       " ('auto repair', 188),\n",
       " ('southern', 186),\n",
       " ('ice cream & frozen yogurt', 178),\n",
       " ('diners', 178),\n",
       " ('nail salons', 177),\n",
       " ('mediterranean', 170),\n",
       " ('delis', 158),\n",
       " ('grocery', 150),\n",
       " ('thai', 148),\n",
       " ('hair salons', 146),\n",
       " ('lounges', 145),\n",
       " ('home & garden', 143),\n",
       " ('local flavor', 143),\n",
       " ('breweries', 140),\n",
       " ('hair removal', 140),\n",
       " ('gluten-free', 137),\n",
       " ('beer bar', 136),\n",
       " ('vegan', 136),\n",
       " ('fashion', 134),\n",
       " ('soup', 129),\n",
       " ('music venues', 126),\n",
       " ('gastropubs', 125),\n",
       " ('juice bars & smoothies', 117),\n",
       " ('vietnamese', 116),\n",
       " ('comfort food', 115),\n",
       " ('pets', 105),\n",
       " ('waxing', 104),\n",
       " ('day spas', 103),\n",
       " ('indian', 102),\n",
       " ('tex-mex', 98),\n",
       " ('greek', 96),\n",
       " ('massage', 94),\n",
       " ('skin care', 93),\n",
       " ('doctors', 93),\n",
       " ('car dealers', 90),\n",
       " ('french', 87),\n",
       " ('middle eastern', 86),\n",
       " ('tacos', 85),\n",
       " ('auto parts & supplies', 84),\n",
       " ('latin american', 83),\n",
       " ('professional services', 81),\n",
       " ('oil change stations', 80),\n",
       " ('food delivery services', 79),\n",
       " ('pet services', 77),\n",
       " ('tapas/small plates', 77),\n",
       " ('tires', 77),\n",
       " ('fitness & instruction', 76),\n",
       " ('real estate', 76),\n",
       " ('tours', 71),\n",
       " ('buffets', 71),\n",
       " ('chicken shop', 69),\n",
       " ('korean', 66),\n",
       " ('noodles', 66),\n",
       " ('soul food', 65),\n",
       " ('hot dogs', 63),\n",
       " ('eyelash service', 63),\n",
       " ('food trucks', 63),\n",
       " ('party & event planning', 61),\n",
       " ('ethnic food', 60),\n",
       " ('caribbean', 59),\n",
       " ('donuts', 57),\n",
       " ('cheesesteaks', 54),\n",
       " ('dive bars', 54),\n",
       " ('parks', 53),\n",
       " ('flowers & gifts', 52),\n",
       " ('bagels', 51),\n",
       " ('furniture stores', 50),\n",
       " ('public services & government', 50),\n",
       " ('contractors', 48),\n",
       " ('museums', 47),\n",
       " ('gyms', 47),\n",
       " ('hair stylists', 46),\n",
       " ('cosmetics & beauty supply', 45),\n",
       " (\"women's clothing\", 45),\n",
       " ('education', 44),\n",
       " ('dentists', 43),\n",
       " ('pet groomers', 43),\n",
       " ('financial services', 42),\n",
       " ('massage therapy', 42),\n",
       " ('drugstores', 42),\n",
       " ('sporting goods', 41),\n",
       " ('tapas bars', 41),\n",
       " ('body shops', 41),\n",
       " ('department stores', 40),\n",
       " ('veterinarians', 40),\n",
       " ('barbers', 40),\n",
       " ('transportation', 39),\n",
       " ('dance clubs', 39),\n",
       " ('jewelry', 39),\n",
       " ('cinema', 38),\n",
       " ('halal', 38),\n",
       " ('makeup artists', 38),\n",
       " ('home decor', 37),\n",
       " ('pet sitting', 37),\n",
       " ('spanish', 37),\n",
       " ('movers', 36),\n",
       " ('electronics', 36),\n",
       " ('bubble tea', 36),\n",
       " ('car wash', 36),\n",
       " ('health markets', 36),\n",
       " ('beer gardens', 35),\n",
       " ('wineries', 35),\n",
       " ('trainers', 35),\n",
       " ('apartments', 35),\n",
       " ('convenience stores', 34),\n",
       " ('pakistani', 34),\n",
       " ('general dentistry', 34),\n",
       " ('auto detailing', 34),\n",
       " ('farmers market', 33),\n",
       " ('landmarks & historical buildings', 33),\n",
       " ('plumbing', 33),\n",
       " ('art galleries', 31),\n",
       " ('cuban', 31),\n",
       " ('car rental', 31),\n",
       " ('accessories', 31),\n",
       " ('fruits & veggies', 31),\n",
       " ('optometrists', 31),\n",
       " (\"men's clothing\", 31),\n",
       " ('dim sum', 31),\n",
       " ('performing arts', 30),\n",
       " ('german', 30),\n",
       " ('brewpubs', 30),\n",
       " ('arts & crafts', 30),\n",
       " ('pet stores', 30),\n",
       " ('wedding planning', 30),\n",
       " ('jazz & blues', 30),\n",
       " ('coffee roasteries', 30),\n",
       " ('medical centers', 30),\n",
       " ('seafood markets', 29),\n",
       " ('wraps', 29),\n",
       " ('cosmetic dentists', 29),\n",
       " ('ramen', 28),\n",
       " ('florists', 28),\n",
       " ('creperies', 28),\n",
       " ('medical spas', 28),\n",
       " ('used car dealers', 28),\n",
       " ('appliances & repair', 27),\n",
       " ('it services & computer repair', 27),\n",
       " ('eyewear & opticians', 27),\n",
       " ('appliances', 27),\n",
       " ('hair extensions', 26),\n",
       " ('heating & air conditioning/hvac', 26),\n",
       " ('food stands', 26),\n",
       " ('hawaiian', 26),\n",
       " ('music & video', 26),\n",
       " ('books', 26),\n",
       " ('mags', 26),\n",
       " ('specialty schools', 25),\n",
       " ('blow dry/out services', 25),\n",
       " ('irish', 25),\n",
       " ('karaoke', 25),\n",
       " ('tanning', 25)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde5697e",
   "metadata": {},
   "source": [
    "We can use <tt>biz_categories_dict</tt> and each review's <tt>business_id</tt> field to check whether each category was used to describe the business.\n",
    "\n",
    "We can do the same for <tt>attributes</tt>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e41c3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caters 5081\n",
      "Alcohol 5210\n",
      "RestaurantsAttire 4863\n",
      "RestaurantsDelivery 5659\n",
      "RestaurantsTakeOut 5825\n",
      "HasTV 5287\n",
      "NoiseLevel 5080\n",
      "BusinessAcceptsCreditCards 7606\n",
      "OutdoorSeating 5486\n",
      "BusinessParking 6851\n",
      "romantic 5265\n",
      "intimate 5265\n",
      "touristy 5265\n",
      "hipster 5265\n",
      "divey 5208\n",
      "classy 5265\n",
      "trendy 5265\n",
      "upscale 5262\n",
      "casual 5265\n",
      "RestaurantsPriceRange2 6703\n",
      "GoodForKids 5550\n",
      "WiFi 5920\n",
      "RestaurantsReservations 5235\n",
      "RestaurantsGoodForGroups 5194\n",
      "CoatCheck 1007\n",
      "Music 1290\n",
      "HappyHour 2786\n",
      "Smoking 846\n",
      "GoodForDancing 983\n",
      "RestaurantsTableService 3209\n",
      "DogsAllowed 3047\n",
      "GoodForMeal 4418\n",
      "BikeParking 6175\n",
      "BestNights 1186\n",
      "ByAppointmentOnly 2080\n",
      "WheelchairAccessible 2855\n",
      "DriveThru 842\n",
      "BusinessAcceptsBitcoin 1642\n",
      "Corkage 736\n",
      "BYOBCorkage 387\n",
      "BYOB 851\n",
      "HairSpecializesIn 44\n",
      "AcceptsInsurance 163\n",
      "Open24Hours 9\n",
      "RestaurantsCounterService 8\n",
      "AgesAllowed 16\n",
      "DietaryRestrictions 9\n"
     ]
    }
   ],
   "source": [
    "for attribute in biz_attributes_dict.keys():\n",
    "    print(attribute, len(biz_attributes_dict[attribute]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978f539e",
   "metadata": {},
   "source": [
    "Let's add a few variables (columns) to our dataframe reflecting different properties of the businesses. Note that with <tt>RestaurantsPriceRange2</tt>, we have to change the datatype from `str` to `float`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eaa192e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"RestaurantsPriceRange2\"] = df.business_id.apply(lambda x: biz_attributes_dict[\"RestaurantsPriceRange2\"].get(x, np.nan))\n",
    "df.loc[df[\"RestaurantsPriceRange2\"] == \"None\", \"RestaurantsPriceRange2\"] = np.nan\n",
    "df = df.astype({\"RestaurantsPriceRange2\": \"float\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37b6ac1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  3., nan,  4.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.RestaurantsPriceRange2.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23de8918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, \"'yes_free'\", \"'no'\", \"'yes_corkage'\", \"u'yes_free'\",\n",
       "       \"u'yes_corkage'\", \"u'no'\"], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"BYOBCorkage\"] = df.business_id.apply(lambda x: biz_attributes_dict[\"BYOBCorkage\"].get(x, np.nan))\n",
    "df.BYOBCorkage.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "daff416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"'no'\": 0, \"'yes_free'\": 1, \"'yes_corkage'\": 1, 'None': np.nan, \"u'no'\": 0}\n",
    "df.BYOBCorkage = df.BYOBCorkage.apply(lambda x: d.get(x, np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9cc52b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.0: 295, 0.0: 323})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df.BYOBCorkage.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "829e4d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From 'Ambience' in the raw data\n",
    "df[\"classy\"] = df.business_id.apply(lambda x: biz_attributes_dict[\"classy\"].get(x, np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "afac848a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>RestaurantsPriceRange2</th>\n",
       "      <th>BYOBCorkage</th>\n",
       "      <th>classy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ziFtaIQdzQfFL79iLTq_zQ</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>not a bad little burger joint. i got the bacon...</td>\n",
       "      <td>2016-11-10 00:35:28</td>\n",
       "      <td>bad little burger joint get bacon cheeseburger...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>skJ1w8B5YusIibyxH4ohxQ</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Our office went today for Friday's happy hour ...</td>\n",
       "      <td>2014-10-11 04:56:46</td>\n",
       "      <td>our office go today friday happy hour special ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zjNcjqgQlPrbnn-pV5YCSg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>High pressure tea sales!  Good teas but outrag...</td>\n",
       "      <td>2012-05-28 22:22:10</td>\n",
       "      <td>high pressure tea sale good tea outrageously e...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O6RCCmz8-x4NhWJpI8KQug</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Food and coffee are great. Just have 30\\nMinut...</td>\n",
       "      <td>2016-07-03 17:13:37</td>\n",
       "      <td>food coffee great just minutes stand line wait...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GBTPC53ZrG1ZBY3DT8Mbcw</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazing! Late night arrival in NOLA so got ama...</td>\n",
       "      <td>2017-12-16 01:20:58</td>\n",
       "      <td>amazing late night arrival nola get amazing se...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  stars  useful  funny  cool  \\\n",
       "0  ziFtaIQdzQfFL79iLTq_zQ      3       0      0     0   \n",
       "1  skJ1w8B5YusIibyxH4ohxQ      5       1      0     1   \n",
       "2  zjNcjqgQlPrbnn-pV5YCSg      1       0      0     0   \n",
       "3  O6RCCmz8-x4NhWJpI8KQug      3       2      1     0   \n",
       "4  GBTPC53ZrG1ZBY3DT8Mbcw      5       0      0     0   \n",
       "\n",
       "                                                text                date  \\\n",
       "0  not a bad little burger joint. i got the bacon... 2016-11-10 00:35:28   \n",
       "1  Our office went today for Friday's happy hour ... 2014-10-11 04:56:46   \n",
       "2  High pressure tea sales!  Good teas but outrag... 2012-05-28 22:22:10   \n",
       "3  Food and coffee are great. Just have 30\\nMinut... 2016-07-03 17:13:37   \n",
       "4  Amazing! Late night arrival in NOLA so got ama... 2017-12-16 01:20:58   \n",
       "\n",
       "                                        preprocessed  RestaurantsPriceRange2  \\\n",
       "0  bad little burger joint get bacon cheeseburger...                     1.0   \n",
       "1  our office go today friday happy hour special ...                     2.0   \n",
       "2  high pressure tea sale good tea outrageously e...                     3.0   \n",
       "3  food coffee great just minutes stand line wait...                     2.0   \n",
       "4  amazing late night arrival nola get amazing se...                     2.0   \n",
       "\n",
       "   BYOBCorkage classy  \n",
       "0          NaN  False  \n",
       "1          NaN   True  \n",
       "2          NaN    NaN  \n",
       "3          NaN  False  \n",
       "4          NaN   True  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "824b99d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.business_id.isin(biz_categories_dict)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb72e6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 11)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "31d01101",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"karaoke\"] = df.business_id.apply(lambda x: \"karaoke\" in biz_categories_dict[x])\n",
    "df[\"breweries\"] = df.business_id.apply(lambda x: \"breweries\" in biz_categories_dict[x])\n",
    "df[\"restaurant\"] = df.business_id.apply(lambda x: \"restaurants\" in biz_categories_dict[x])\n",
    "df[\"coffee\"] = df.business_id.apply(lambda x: \"coffee\" in biz_categories_dict[x])\n",
    "df[\"breakfast\"] = df.business_id.apply(lambda x: \"breakfast & brunch\" in biz_categories_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b1fbbfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"restaurant2\"] = df.business_id.apply(lambda x: \"restaurants\" in biz_categories_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2bcfc2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>restaurant2</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restaurant</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>3165</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>0</td>\n",
       "      <td>6835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "restaurant2  False  True\n",
       "restaurant              \n",
       "False         3165     0\n",
       "True             0  6835"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df.restaurant, df.restaurant2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ab5290",
   "metadata": {},
   "source": [
    "## II. Sentiment with VADER\n",
    "\n",
    "To expand our toolkit a bit more in general and increase the range of variables we can explore in this notebook in particular, we are going to use VADER to analyze sentiment. You can check out the original paper here: [VADER: A Parsimonious Rule-based Model for  Sentiment Analysis of Social Media Text](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf). You can read more about it on the GitHub page [here](https://github.com/cjhutto/vaderSentiment).\n",
    "\n",
    "VADER provides positive, neutral, and negative sentiment scores as well as a 'compound' score. According to the [GitHub page](https://github.com/cjhutto/vaderSentiment):\n",
    "> The compound score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate.\n",
    "\n",
    "We'll focus on the compound score in our analyses. Let's print a couple of example reviews, the scores from VADER, and the number of stars the reviewer assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f9bdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca3eb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.sample(2).iterrows():\n",
    "    print(f\"Review {idx}\\n--\")\n",
    "    print(row.text)\n",
    "    vs = analyzer.polarity_scores(row.text)\n",
    "    print(vs)\n",
    "    print(f\"Stars: {row.stars}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044fe0cd",
   "metadata": {},
   "source": [
    "...and let's go ahead and compute the scores for each review in our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7827e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df[[\"neg\", \"neu\", \"pos\", \"compound\"]] = df.preprocessed.apply(lambda x: pd.Series(analyzer.polarity_scores(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dac2a4",
   "metadata": {},
   "source": [
    "We'll also a variable for the day of the week that could be useful for focusing on reviews of particular types of events (e.g., dinner on Friday vs. Sunday brunch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0db3032",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df[\"day_name\"] = df[\"date\"].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741dd593",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e469db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"yelp_sample.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550be32c",
   "metadata": {},
   "source": [
    "## III. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398169ab",
   "metadata": {},
   "source": [
    "Pandas dataframes have a `describe` method that provides basic [descriptive statistics](https://en.wikipedia.org/wiki/Descriptive_statistics). You can also compute them directly (e.g., by calling `.mean()` or `.median()`) or by using a library like NumPy directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf51880",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"stars\", \"neg\", \"neu\", \"pos\", \"compound\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dc163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean: {np.mean(df.stars):.2f}\")\n",
    "print(f\"Mean: {np.median(df.stars):.2f}\")\n",
    "print(f\"Min: {np.min(df.stars):.2f}\")\n",
    "print(f\"Max: {np.max(df.stars):.2f}\")\n",
    "print(f\"Standard deviation (using n): {np.std(df.stars):.5f}\") # default: formula for population standard deviation\n",
    "print(f\"Standard deviation (using n-1): {np.std(df.stars, ddof=1):.5f}\") # use n-1 for sample standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8540f0b2",
   "metadata": {},
   "source": [
    "## IV. Inferential Statistics\n",
    "\n",
    "Inferential statistics is all about using what we know about a sampleâ€”including variability in the data, which is a source of uncertaintyâ€”to make inferences about the population from which a sample was drawn.\n",
    "\n",
    "[Benjamin et al. (2018)](https://www.nature.com/articles/s41562-017-0189-z?source=post_page---------------------------) describe the typical approach to inferential statistics:\n",
    "\n",
    "> In testing a point null hypothesis H<sub>0</sub> against an alternative hypothesis H<sub>1</sub> based on data x<sub>obs</sub>, the P value is defined as the probability, calculated under the null hypothesis, that a test statistic is as extreme or more extreme than its observed value. The null hypothesis is typically rejected â€” and the finding is declared statistically significant â€” if the P value falls below the (current) [type I error](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors) threshold Î± = 0.05.\n",
    "\n",
    "Put differently, a p-value is our attempt to quantify the likelihood that we would observe the data we actually observe (including the test statistic derived from the data) if the null hypothesis were true. A low p-value suggests the likelihood of observing the data should be low *if the null hypothesis is correct*. We typically use a threshold like 0.05 as the cutoff for declaring statistical significance. If the p-value is >= 0.05, we *retain* the null hypothesis; if the p-value is < 0.05, we *reject* the null hypothesis.\n",
    "\n",
    "We also refer to the 95% confience level (1 - 0.05) when Î± = 0.05. This is the conventional threshold, but Benjamin et al. (2018) argue against this for various reasons. [Consider this XKCD comic](https://xkcd.com/882/), for example.\n",
    "\n",
    "The null hypothesis (H<sub>0</sub>) and the alternative hypothesis (usually referred to as H<sub>1</sub> or H<sub>A</sub>) will depend on the specific test.\n",
    "\n",
    "#### Correlation\n",
    "\n",
    "Correlation refers to the strength and direction of the (potential) relationship between two variables. Usually, we assume this relationship is pretty much linear. A positive correlation coefficient indicates that when one variable increases, the other does, too. A negative correlation implies that as one variable increases, the other instead decreases.\n",
    "\n",
    "Correlation is sometimes considered to be part of descriptive statistics because correlation coefficients *describe* the strength of a relationship between two variabies. However, when we calculate a correlation coefficient, we also typically performance a hypothesis test, which is part of inferential statistics.\n",
    "\n",
    "If we calculate the correlation coefficient between two continuous variables, our null hypothesis is that the correlation is 0 (i.e., that the variables are uncorrelated). A significant correlation coefficient suggests the variables are correlated at some confidence level (determined by the p-value)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7fd3f6",
   "metadata": {},
   "source": [
    "You can calculate correlation coefficients directly using the [`.corr()` method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html) with your dataframe. By default, this will produce a correlation matrix using Pearson's *r*. You can add the \"spearman\" argument for Spearman's rho or \"kendall\" for the Kendall rank correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dadd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"stars\", \"neg\", \"neu\", \"pos\", \"compound\", \"RestaurantsPriceRange2\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caa7b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"stars\", \"neg\", \"neu\", \"pos\", \"compound\", \"RestaurantsPriceRange2\"]].corr(\"spearman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe4b78",
   "metadata": {},
   "source": [
    "We can also save the whole correlation matrix as a variable (here, <tt>correlation_matrix</tt>) and plot it using Seaborn's `heatmap` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df[[\"stars\", \"neg\", \"neu\", \"pos\", \"compound\"]].corr()\n",
    "sns.heatmap(correlation_matrix, cmap=\"coolwarm_r\", annot=True, square=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b558ed78",
   "metadata": {},
   "source": [
    "The `pingouin` library adds features like the [`rcorr` method](https://pingouin-stats.org/generated/pingouin.rcorr.html), which display correlation coefficients in the lower triangle and significance levels (represented by asterisks) in the upper triangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28f6c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"stars\", \"compound\", \"RestaurantsPriceRange2\"]].rcorr(\"spearman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8997530e",
   "metadata": {},
   "source": [
    "`scipy.stats` makes it easy to access the correlation coefficient and p-value for specific pairs of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac838f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(df.stars, df.compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49cc51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(df.stars, df.compound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563601da",
   "metadata": {},
   "source": [
    "We can access the individual pieces of information (the correlation coefficient *r* and the p-value) by assigning the results to two variables (<tt>r</tt> and <tt>pvalue</tt>) separated by a comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2e0f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r, pvalue = pearsonr(df.stars, df.compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c078cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084e28c8",
   "metadata": {},
   "source": [
    "**Note: The p-value is being rounded down to zero. It's not precisely zero. We will never be that certain if we are only looking at a sample.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a06df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bcb608",
   "metadata": {},
   "source": [
    "#### Comparing group means with *t*-tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f9fab6",
   "metadata": {},
   "source": [
    "To test for differences in the means between two groups, we'll use the `statsmodels` implementation of independent samples *t*-tests, `ttest_ind`. Specifically, we are going to test whether restaurants categorized as breweries and restaurants categorized as having karaoke have different ratings on average.\n",
    "\n",
    "First, we want to make sure that our groups do not overlap (i.e., that no restaurant in our sample is a brewery that does karaoke). The code below checks whether there are any reviews for businesses that are describes as breweries and as having karaoke. There should be none in the sample.\n",
    "\n",
    "Next we'll compare the mean star-rating for each of these groups and then conduct a *t*-test. `ttest_ind` returns the *t*-statistic, the *p*-value, and another piece of information we won't get into called the degrees of freedom. In this sample, there is not a significant difference in the mean star-rating of breweries and businesses offering karaoke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3278a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.karaoke==True) & (df.breweries==True)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57192b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df.karaoke==True].stars.mean())\n",
    "print(df[df.breweries==True].stars.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3577a386",
   "metadata": {},
   "source": [
    "On average, breweries get an extra 0.25 stars, but the p-value is greater than 0.05, so we *retain* (or *fail to reject*) the null hypothesis. In other words, we would conclude there is no evidence of a difference between these groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c71f784",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df.breweries==True].stars.mean()-df[df.karaoke==True].stars.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2410be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t, pvalue, _ = ttest_ind(df[df.karaoke==True].stars, df[df.breweries==True].stars, usevar=\"unequal\")\n",
    "print(f\"t = {t:.2f}, p = {pvalue:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e0aca",
   "metadata": {},
   "source": [
    "How about restaurants that are described as classy or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1081cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df.classy==True].stars.mean())\n",
    "print(df[df.classy==False].stars.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a716523",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df.classy==True].stars.mean()-df[df.classy==False].stars.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbba238",
   "metadata": {},
   "outputs": [],
   "source": [
    "t, pvalue, _ = ttest_ind(df[df.classy==True].stars, df[df.classy==False].stars, usevar=\"unequal\")\n",
    "print(f\"t = {t:.2f}, p = {pvalue:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b47f8a8",
   "metadata": {},
   "source": [
    "On average, restaurants described as classy receive an extra 0.27 stars. The p-value has been rounded down to zero, and isn't actually zero; but we can conclude that the group means are significantly different at the p < 0.01 level (or 99% confidence level)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21afe5c",
   "metadata": {},
   "source": [
    "#### Linear Regression with OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2de869",
   "metadata": {},
   "source": [
    "Now we will turn to linear regression using ordinary least squares (OLS). OLS tests the strength and significance of the relationship between the dependent variable ('outcome' or 'response' variable) and each independent variable ('predictor' or 'explanatory variable') in a way that can seem more flexible and interpretable than the previous tests. Whereas correlation coefficients are 'unit-free', a regression coefficient is interpreted as the unit change in the y variable for a unit change in the x variable. The intercept (or 'constant') in the model is the conditional mean of the dependent variable.\n",
    "\n",
    "We are going to use the [`statsmodels` library](https://www.statsmodels.org/stable/index.html) to train our regression models and the [`stargazer` library](https://github.com/mwburke/stargazer) to format regression tables. <tt>compound</tt> is a continuous variable ranging between -1 and 1. <tt>RestaurantsPriceRange2</tt> is an ordinal variable ranging from 1 to 4, though we can treat it as a continuous variable. <tt>breakfast</tt> and <tt>classy</tt> are stored as `bools` (True/False), and `statsmodels` will implicitly treat them as categorical variables where `False` == 0 and `True` == 1. This means that they will be treated as dummy variables (another name for binary or dichotomous variables), and the coefficient for each variable indicates the change in the outcome when the variable is `True` (or 1), as opposed to `False` (or 0).\n",
    "\n",
    "More specifically, we are going to use `statsmodels.formula.api`, which allows us to write our regression equations using R-like syntax: e.g., `y ~ x1 + x2`. See [here](https://www.statsmodels.org/devel/example_formulas.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9769dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to create a subset of the dataframe without missing data for the variables in our models\n",
    "tmp = df[[\"compound\", \"RestaurantsPriceRange2\", \"breakfast\", \"classy\"]].dropna()\n",
    "\n",
    "formula = \"compound ~ RestaurantsPriceRange2 + breakfast\"\n",
    "ols1 = smf.ols(formula, data=tmp).fit()\n",
    "\n",
    "formula = \"compound ~ RestaurantsPriceRange2 + breakfast + classy\"\n",
    "ols2 = smf.ols(formula, data=tmp).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a1721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_table = Stargazer([ols1, ols2])\n",
    "reg_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1c44cb",
   "metadata": {},
   "source": [
    "`stargazer` offers numerous methods for editing the appearance of the table, for example the order the independent variables appear in, the names of the independent variables, and the title of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf11c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_table = Stargazer([ols1, ols2])\n",
    "reg_table.covariate_order([\"RestaurantsPriceRange2\", \"breakfast[T.True]\", \"classy[T.True]\", \"Intercept\"])\n",
    "reg_table.rename_covariates({\"RestaurantsPriceRange2\": \"Price\", \"breakfast[T.True]\": \"Breakfast\", \"classy[T.True]\": \"Classy\"})\n",
    "reg_table.title(\"Compound Sentiment Regressed on Price, Ambience, and Breakfast\")\n",
    "reg_table.show_model_numbers(False)\n",
    "reg_table.significance_levels([0.05, 0.01, 0.001])\n",
    "reg_table.custom_columns([\"Model 1\", \"Model 2\"], [1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424f4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a04899c",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "For each tier increase in the measure of the restaurant's price range, we would expect the <tt>compound</tt> sentiment score of a review to increase by 0.088 on average according to Model 1 or by 0.062 on average according to Model 2, net of the other variables in each model.\n",
    "\n",
    "Serving breakfast is associated with an increase in the <tt>compound</tt> sentiment score of approximately 0.039 (Model 1) or 0.031 (Model 2), net of the other variables in each model.\n",
    "\n",
    "Businesses characterized as having a \"classy\" ambience score 0.073 higher on the <tt>compound</tt> score of sentiment on average, net of price and whether or not the business serves breakfast.\n",
    "\n",
    "Based on a comparison of the coefficients in these two models, it appears that controlling for whether a restaurant is categorized as <tt>classy</tt> accounts for some of the association between the outcome and the other independent variables. For example, the coefficient on price decreases from 0.088 to 0.062 when we control for <tt>classy</tt>. This could be interpreted in different ways.\n",
    "\n",
    "For example, classiness may be associated both with higher prices and with factors that lead to more positive reviews (i.e., higher <tt>compound</tt> scores). This would be an example of confounding. Alternatively, it may be that higher prices make a restaurant seem more classy, which in turn leads to more positive reviews. This would be an example of partial mediation, and there are more formal ways to test for this. The larger point is that this is observational data and as much as we may like to offer causal interpretations, we can't be certain. Our job when using tools like linear regression is to come up with the best hypothesis we can, motivated by the best theory, and then use our models (and the results of significance tests) as components of an argument.\n",
    "\n",
    "In OLS, each coefficient is being tested using the null hypothesis (H<sub>0</sub>) that the coefficient is equal to zero. A significant coefficient on any of the independent variables indicates we can reject the null hypothesis and conclude that the two variables are related in some way. The coefficient quantifies the strength of that relationship.\n",
    "\n",
    "Note that both the R<sup>2</sup> and Adjusted R<sup>2</sup> increase from Model 1 to Model 2. The R<sup>2</sup> is a measure of how much variance in the outcome is explained by the independent (or explanatory) variables. This tends to increase if we add additional independent variables. The adjusted R<sup>2</sup> is penalized for each additional variable, so it is slightly lower the plain R<sup>2</sup> in Model 2.\n",
    "\n",
    "A **crucial point** with this kind of statistical modeling is that we ultimately should not care much about the R<sup>2</sup> or Adjusted R<sup>2</sup>. If we are interested in the relationship between some variable x and another variable y, we want to include additional variables only if we expect that they confound the relationship (i.e., have an effect on both x and y) or, more generally, are a source of [omitted-variable bias](https://en.wikipedia.org/wiki/Omitted-variable_bias). If you want a deeper dive into these issues, see [this paper](https://journals.sagepub.com/doi/full/10.1177/0049124118782542), although it is a reply to a comment on another paper, so a lot of the original context won't be immediately clear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdef751",
   "metadata": {},
   "source": [
    "With Stargazer, can also produce LaTeX code for the table directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg_table.render_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764b9fed",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "Finally, we will take a quick look at an example of logistic regression using `statsmodels`. Coefficients are reported in log-odds, but interpretations are otherwise similar: A unit change in x is associated with a change in the log-odds of y of some amount.\n",
    "\n",
    "For our dependent variable, we'll use the BYOBCorkage attribute, which we have dichotomized to no == 0 and yes == 1. What factors affect whether the restaurant will uncork your BYOB wine for you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc55aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to create a subset of the dataframe without missing data for the variables in our models\n",
    "tmp = df[[\"BYOBCorkage\", \"RestaurantsPriceRange2\", \"breakfast\", \"classy\"]].dropna()\n",
    "\n",
    "formula = \"BYOBCorkage ~ RestaurantsPriceRange2 + breakfast\"\n",
    "logit1 = smf.logit(formula, data=tmp).fit()\n",
    "\n",
    "formula = \"BYOBCorkage ~ RestaurantsPriceRange2 + breakfast + classy\"\n",
    "logit2 = smf.logit(formula, data=tmp).fit()\n",
    "\n",
    "logit_table = Stargazer([logit1, logit2])\n",
    "logit_table.covariate_order([\"RestaurantsPriceRange2\", \"breakfast[T.True]\", \"classy[T.True]\", \"Intercept\"])\n",
    "logit_table.rename_covariates({\"RestaurantsPriceRange2\": \"Price\", \"breakfast[T.True]\": \"Breakfast\", \"classy[T.True]\": \"Classy\"})\n",
    "logit_table.title(\"BYOBCorkage Regressed on Price, Ambience, and Breakfast\")\n",
    "logit_table.show_model_numbers(False)\n",
    "logit_table.significance_levels([0.05, 0.01, 0.001])\n",
    "logit_table.custom_columns([\"Model 1\", \"Model 2\"], [1, 1])\n",
    "logit_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d902cf8",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "According to the first model, an increase in the price range of the business is associated with a 0.298 increase in the log-odds that the business will uncork your BYOB wine. When we control for whether the restaurant is considered classy (Model 2), we no longer observe a significant relationship between price and the outcome.\n",
    "\n",
    "Note that the lines for R<sup>2</sup> and Adjusted R<sup>2</sup> are blank for the logistic regression table. We cannot calculate a true R<sup>2</sup> or Adjusted R<sup>2</sup> using logistic regression. There are alternatives that try to quantify how well the model fits the data, but they aren't implemented here.\n",
    "\n",
    "## V. Supervised Machine Learning\n",
    "\n",
    "Whereas inferential statistics is concerned with generalizing from a sample to a population by quantifying uncertainty, and we often want to know about the relationship between two specific variables net of any potential confounders, that's all (usually) beside the point in supervised machine learning. We want to train models that predict outcomes well, regardless of the substantive importance of the relationship in question. Confounding is no longer an issueâ€“we just want to predict the outcome well and in a way that generalizes to unseen data.\n",
    "\n",
    "One thing you may noticeâ€“and which may cause some confusionâ€“is that tools we use for inferential statistics can be used for supervised machine learning and vice versa. The two biggest categories of machine learning are *regression* (predicting continuous outcomes, like income or ratings) and *classification* (predicting membership in a category). We used OLS for linear regression in the inferential statistics section, and we can use OLS for supervised machine learning as well. Further, we used logistic regression for inferential statistics, and we can use it for classification tasks in the context of supervised machine learning.\n",
    "\n",
    "#### Regression\n",
    "\n",
    "In regression tasks in a supervised machine learning framework, we want to minimize an objective function like the mean squared error or root mean squared error. [Mathematically, this is equivalent to *maximizing* the R<sup>2</sup>](https://stats.stackexchange.com/questions/250730/what-is-the-mathematical-relationship-between-r2-and-mse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0104e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df.preprocessed.tolist()\n",
    "y = df.compound.tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(docs, y, test_size=0.3, random_state=8675309)\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "regr = LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33cdb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Coefficients: \\n{regr.coef_}\")\n",
    "print(f\"Mean squared error: {mean_squared_error(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e5d41c",
   "metadata": {},
   "source": [
    "#### Classification\n",
    "\n",
    "In classification problems, two major properties of our models are *precision* and *recall*. The diagram below clarifies the difference between the two. In practice, people often default to using an [F-score](https://en.wikipedia.org/wiki/F-score), such as F<sub>1</sub>, which is the harmonic mean of precision and recall.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/soc128d/soc128d.github.io/master/assets/images/precision_recall_wiki_walber_side_by_side.png\" width=800 align=\"left\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094ff3d9",
   "metadata": {},
   "source": [
    "([Image source](https://en.wikipedia.org/wiki/F-score#/media/File:Precisionrecall.svg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c382ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.compound.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f7b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"compound_binary\"] = df.compound.apply(lambda x: x >= df.compound.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed8e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.compound_binary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b8a54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "docs = df.preprocessed.tolist()\n",
    "labels = df.compound_binary.tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(docs, labels, test_size=0.3, random_state=8675309)\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(fit_intercept=False, solver=\"sag\", penalty=\"l2\", max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad09d69",
   "metadata": {},
   "source": [
    "You can read more about [`classification_report` here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da70a933",
   "metadata": {},
   "source": [
    "## VI. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b90b9ea",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    For the exercises in this notebook, you will use the <tt>categories</tt> and <tt>attributes</tt> data to create new variables to answer a social research question. You will then use t-tests or correlation coefficients and finally a linear or logistic regression to test your hypothesis. <br><br>\n",
    "    <b>Exercise 1</b><br><br>\n",
    "    Keeping in mind the variables you will create, what is your research question? You might find it helpful to pose your question in terms of the relationship between two variables. At least one of these should be a continuous variable (or a variable that can be treated as continuous). It may also be helpful to use one of the pre-existing continuous variables (e.g., star ratings or sentiment) as the outcome (dependent variable). Why did you choose these variables? Why might they be related? Are there variables that you may need to control for in order to model the relationship between your two variables? You might also consider using a subset of the data confined to a particular period of time. If needed, you can start with a sample larger than the 10,000-review sample taken at the start of the notebook. (You may also find it helpful to wait to fully finish this question until you've made sure you can create the variables you have in mind.)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3ccf86",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa377daa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2</b><br><br>\n",
    "    2.1 Now create two variables. If you are using one of the pre-existing variables as your independent or (more likely) dependent variable, you might create the other variable and then create an additional variable to use as a control variable. The variables should fit your research question. <br><br>\n",
    "    Remember, you can create variables using code like the following: </div>\n",
    "    \n",
    "```python\n",
    "df[\"classy\"] = df.business_id.apply(lambda x: biz_attributes_dict[\"classy\"].get(x, np.nan))\n",
    "```\n",
    "\n",
    "```python\n",
    "df[\"karaoke\"] = df.business_id.apply(lambda x: \"karaoke\" in biz_categories_dict[x])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3872421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188ac019",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "2.2 Be sure to check the values of your new variables and recode them if needed. You can using code like <tt>df.NEW_VARIABLE.unique()</tt> to display the unique values.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e824f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e9cbac",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3</b><br><br>\n",
    "    3.1 If your independent and dependent variables are both continuous (or can be treated as continuous, e.g. <tt>RestaurantsPriceRange2</tt>), are they correlated with one another? Use <tt>pearsonr</tt> or <tt>spearmanr</tt> from <tt>scipy.stats</tt> and report both the correlation coefficient and p-value. If only one of these variables is continuous, then select two groups from your categorical variable (e.g., the True and False groups) to compare with respect to the continuous variable. What are the group means of the continuous variable? Is one mean substantially higher? Is there a significant difference according to a t-test? Use <tt>ttest_ind</tt>. Report the group means, the difference, the results of the t-test. What do the results say about your research question? Do they fit with your expectations?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4dcc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08533d61",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db4db93",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    3.2 Now, assuming your dependent variable is continuous, train at least two linear regression models using OLS (and <tt>smf.ols</tt>). If your dependent variable is categorical, then make sure you are using a binary version of it (i.e., only two groups) and train at least two logistic regression models using <tt>smf.logit</tt> instead. The models should be trained using the same observations and should only differ in that the second (or third, etc.) model has one additional control variable. The addition of a control variable may reflect what you write in Exercise 1. You can train the models on the same subset of observations by using code like the code below to exclude any observations missing data for any of the variables you will use:</div>\n",
    "    \n",
    "```python\n",
    "tmp = df[[\"compound\", \"RestaurantsPriceRange2\", \"breakfast\", \"classy\"]].dropna()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba9dba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0de62f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    3.2 Now use <tt>stargazer</tt> to make a regression table presenting the results of your models.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c9462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75eead0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    3.3 Interpret the results. Do they fit your expectations? What do they tell us about your research question?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad82854",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
